<head>
    <title>Mingfei Han @MBZUAI</title>
    <meta name="author" content="Mingfei Han">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Mingfei Han@UTS">
	<meta property="og:description" content="Postdoctoral Associate, MBZUAI; Ph.D., University of Technology Sydney">
    <meta property="og:image" content="https://mingfei.info/files/me.jpeg">
	<meta property="og:url" content="https://mingfei.info/">
<!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/uts-logo.png">
    <link rel="icon" type="image/png" href="files/uts-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Mingfei Han</h1>
            </div>
            <div class="header-subtitle">
                Ph.D., University of Technology Sydney
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=wJEoIXsAAAAJ&hl=en">Google Scholar</a> /
                <a class="btn" href="https://github.com/youthHan">GitHub</a> /
                <a class="btn" href="files/resume.pdf">Resume</a> /
                <a class="btn" href="https://www.linkedin.com/in/mingfei-han/">LinkedIn</a>
            </div>
        </div>
    </div>
</div>
<style>
    .section-spacing {
        margin-bottom: 10px; /* Adjusts space between sections */
    }

    .list-item-spacing {
        margin-bottom: 5px; /* Adjusts space between list items */
    }
</style>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am a postdoctoral researcher at <a href="https://mbzuai.ac.ae/">Mohamed Bin Zayed University of Artificial Intelligence</a>. I obtained my Ph.D. degree from <a href="https://www.uts.edu.au/">University of Technology Sydney</a>, advised by <a href="https://xiaojun.ai/">Prof. Xiaojun Chang</a>. I also worked closely with <a href="https://hengcv.github.io/">Heng Wang</a>, <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>, and <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a> on various video-language projects at Bytedance Seed. 
            Before moving to UTS, I spent a wonderful two years at <a href="https://monash.edu/">Monash University</a>. Prior to my candidature, I was a visiting student at <a href="https://mmlab.siat.ac.cn/">MMLab, SIAT, Chinese Academy of Sciences</a>, where I was fortunate to work with <a href="https://http://mmlab.siat.ac.cn/yuqiao/">Prof. Yu Qiao</a>, and <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ&hl=zh-CN">Prof. Yali Wang</a>.<br/>
            I received my Master's degree from the University of Chinese Academy of Sciences (<a href="https://english.ucas.ac.cn/">UCAS</a>) and my Bachelor's degree from Nankai University (<a href="https://en.nankai.edu.cn/">NKU</a>) with graduate honours.
        </p>
		<p>
		<span class="italic">I <span style="text-decoration: underline;">welcome collaborations</span> with both industry and academia. There are also <span style="text-decoration: underline;">openings for self-motivated, active-thinking MSc and PhD students</span> (relevant background and publications are a plus) in the groups at MBZUAI and USTC.</span>
		</p>
		<p>
		<span class="italic">‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶Êúâ26Âπ¥ÂÖ•Â≠¶ÁöÑÁ°ïÂ£´ÂêçÈ¢ùÔºåÊÄ•ÊãõÔºåËØ∑ÊÑüÂÖ¥Ë∂£ÁöÑÂêåÂ≠¶ÈÇÆ‰ª∂ËÅîÁ≥ª <span class="bold">mhannku030</span> [at] <span class="bold">gmail</span>.com. ÈïøÊúüÂºÄÊîæÊé®ÂÖç‰∏éÁ°ïËΩ¨ÂçöÊú∫‰ºö.</span>
		</p>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Recent Activities</h2>
	    <ul>
<!-- 		<li class="list-item-spacing" style="color: #ff4500;">
		    <span role="img" aria-label="Star">üåüüåü</span> <strong>I am currently on the job market for a research position.</strong> <span role="img" aria-label="Star">üåüüåü</span>
		</li> -->
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">AAAI 2026</span>: Two papers are accepted on zero-shot image inpainting and referring audio-visual segmentation. Congratulations to my collaborators!</li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">IROS 2025</span>: We won the first place awards in <a href="https://internrobotics.shlab.org.cn/challenge/2025/">Intern Robotics Challenge Track#2 (Vision-and-Language Navigation in Physical Environments)</a> and <a href="https://robosense2025.github.io/track2">RoboSense Challenge Track#2 (Social Navigation)</a>! </li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">NeurIPS 2025</span>: Our <a href="https://johanan528.github.io/worldweaver_web/">WorldWeaver</a> and <a href="https://phyblock.github.io/">PhyBlock</a> are accepted to NeurIPS 2025! </li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">IROS 2025!</span> <a href="https://malmm1.github.io/">MALMM</a>: Extendable and Effective multi-agent framework for robotics manipulation.</li>
		<li class="list-item-spacing" ><a href="https://robomani-grail.github.io/" style="color: #f09228; text-decoration: none; font-weight: bold;">üåüGRAIL Challenge & Workshopüåü</a>-<a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>: Benchmarking generalization in robotics manipulation with held testing set and offline real-world setup.</li>
		<li class="list-item-spacing" ><a href="https://smm-challenge.github.io/" style="color: #f09228; text-decoration: none; font-weight: bold;">üåüSMM Challengeüåü</a>-<a href="https://embodied-ai.org/cvpr2025/">EAI Workshop CVPR 2025</a>: Benchmarking the capability of performing long-sequence complex tasks through social interactions.</li>
		<li class="list-item-spacing" ><a href="https://arxiv.org/pdf/2412.08591/" style="color: #f09228; text-decoration: none; font-weight: bold;">üåüCVPR 2025üåü</a> <a href="https://roomtour3d.github.io/">RoomTour3D</a>: Diverse and Scalable video-instruction(-action) data for embodied navigation (Vision-and-Language Navigation). Ongoing effort with current 200k instructions released. We achieve SOTA on SOON and REVERIE with this newly introduced data.</li>
		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/" style="color: #f09228; text-decoration: none; font-weight: bold;">üåüICLR 2025üåü</a> <a href="https://mingfei.info/shot2story/">Shot2Story</a>: Train a rich video summarizer and test it on multi-shot videos! Just release our manually annotated Multi-Shot Video Understanding Suite. Single-shot captions, Mult-shot summaries, Video question-answering pairs: 134K multishot videos covering over 548k video shots; Detailed text summaries with over 6M words! SOTA video QA performance with the derived rich text summaries.</li>
<!-- 		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/">Shot2Story-134K</a>: Manual 43K + GPTV 90K. 134K multishot videos covering over 548k video shots; Detailed text summaries with over 6M words! We have released this new video description dataset. With the assistance of LLM, our method achieves SOTA performance on zero-shot MSRVTT-QA.</li> -->
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">ECCV 2024 Oral</span> <a href="https://github.com/ziplab/LongVLM">LongVLM</a>: Efficient long-video frame encoding for large video language models.</li>
		<!-- <li class="list-item-spacing" >CVPR 2024 <a href="https://mingfei.info/PMV/">PMV-400</a>: Portrait-mode videos rock the social media! We have developed the first video dataset dedicated to the research of this emerging video format.</li> -->
<!-- 		<li class="list-item-spacing" >ICCV 2023 <a href="https://mingfei.info/HTML/">HTML</a>: One paper on language referring video object segmentation (RVOS) gets accepted. No additional cost during inference with performance largely boosted!</li> -->
<!-- 		<li class="list-item-spacing" >NeurIPS 2023: One paper on efficient video segmentation gets accepted.</li> -->
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
	<div class="section-spacing" style="max-width: 900px;">
	    <h2 class="noselect" style="margin-bottom: 12px;">Research Interest</h2>
	    
	    <p style="margin-bottom: 15px; line-height: 1.6; text-align: justify;">
	        My research operates at the intersection of Computer Vision and Robotics, driven by the mission to architect <span class="bold">General-Purpose Physical Intelligence</span>. We investigate how foundational multimodal reasoning can be seamlessly integrated with physical priors, world dynamics, and geometric constraints. By bridging the gap between passive visual understanding and active embodied agency, our goal is to empower autonomous systems to perceive, reason, and act reliably within complex, unstructured environments.
	    </p>
	
	    <p style="margin-bottom: 10px; font-weight: 500;">
	        Our group explores three synergetic pillars that combine neural architectures with structured intelligence:
	    </p>
	
	    <ul style="list-style-type: none; padding-left: 0; margin-top: 0;">
	        <li style="margin-bottom: 8px; line-height: 1.5; display: flex; align-items: flex-start;">
	            <span style="min-width: 105px; font-weight: bold; display: inline-block;">Action (Core):</span>
	            <span style="flex: 1;">Pioneering paradigms for navigation and manipulation by fusing web-scale video priors with world models and geometric representations.</span>
	        </li>
	        <li style="margin-bottom: 8px; line-height: 1.5; display: flex; align-items: flex-start;">
	            <span style="min-width: 105px; font-weight: bold; display: inline-block;">Perception:</span>
	            <span style="flex: 1;">Developing reliable and efficient systems for complex temporal reasoning, long-form video understanding, and transparent decision-making.</span>
	        </li>
	        <li style="margin-bottom: 0px; line-height: 1.5; display: flex; align-items: flex-start;">
	            <span style="min-width: 105px; font-weight: bold; display: inline-block;">Interaction:</span>
	            <span style="flex: 1;">Advancing communities for social awareness and multi-agent collaboration through initiatives like the CVPR 2025 SMM & GRAIL challenges.</span>
	        </li>
	    </ul>
	</div>
    <div>
        <h2 class="noselect">Publications and preprints</h2>
		(* Project lead and adviser. ‚Ä† Corresponding author.)
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/roomtour_igr.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Implicit Geometry Representations for Vision-and-Language Navigation from Web Videos</a><br/>
                <span class="bold">Mingfei Han</span>, Haihong Hao, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang and Ivan Laptev<br/>
                <span class="italic">In submission</span>, 2025 <br/>
				<span class="italic">Implicit geometric prior from YouTube RoomTour videos at low cost, powered by our scalable data curation pipeline and unified geometry perception architecture for strong VLN performance across multiple benchmarks and zero-shot setup. </span> <br/>
                <!-- <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a>   -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/thinking_qwenvl.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Progressive Online Video Understanding with Evidence-Aligned Timing and Transparent Decisions</a><br/>
                Kecheng Zhang, Zongxin Yang, <span class="bold">Mingfei Han‚Ä†</span>, Haihong Hao, Yunzhi Zhuge, Changlin Li, Junhan Zhao, Zhihui Li and Xiaojun Chang<br/>
                <span class="italic">ICLR</span>, 2026 <br/>
				<span class="italic">A proactive VLM agent with evidence-aligned and trasnparent decision making process. Our Thinking-QwenVL provides timely, accurate streaming decisions and achieves state-of-the-art performance on diverse online and offline video benchmarks. </span> <br/>
                <!-- <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a>   -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/geosense.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">GeoSense: Internalizing Geometric Necessity Perception for Multimodal Reasoning</a><br/>
                <span class="italic">Ruiheng Liu</span>, <span class="italic">Haihong Hao</span>, <span class="italic"><span class="bold">Mingfei Han*</span></span>, Xin Gu, Kecheng Zhang, Changlin Li, Xiaojun Chang<br/>
                <!-- <span class="italic">Tingjun Dai</span>, <span class="italic"><span class="bold">Mingfei Han</span></span>, Tingwen Du, Zhiheng Liu, Zhihui Li, Salman Khan, Jun Yu and Xiaojun Chang<br/> -->
                <span class="italic">TBR</span>, 2026 <br/>
				<span class="italic">External geometry information matters, but only when needed. We propose to internalize the necessity perception capability into MLLMs for effective and universal multimodal reasoning. Our spatial reasoner functions as an accurate ruler, a selective verifier, and a resilient generalist.</span> <br/>
                <!-- <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a>   -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/spr_vla.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">See, Plan, Rewind: Progress-Aware Vision-Language-Action Models for Robust Robotic Manipulation</a><br/>
                <span class="italic">Tingjun Dai</span>, <span class="italic"><span class="bold">Mingfei Han*</span></span>, Tingwen Du, Zhiheng Liu, Zhihui Li, Salman Khan, Jun Yu and Xiaojun Chang<br/>
                <span class="italic">TBR</span>, 2025 <br/>
				<span class="italic">Progress awareness matters! By seeing remaining subtasks and planning toward spatial subgoals, SPR enables progress-aware robotic manipulation with autonomous error recovery, achieving state-of-the-art out-of-distribution robustness. </span> <br/>
                <!-- <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a>   -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/sp_vla.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Beyond Next Frames: World Models as Structured Planners for Robotic Manipulation</a><br/>
                <span class="italic">Minghao Jin</span>, <span class="italic">Mozheng Liao</span>, <span class="italic"><span class="bold">Mingfei Han*</span></span>, Zhihui Li, Xiaojun Chang<br/>
                <span class="italic">TBR</span>, 2025 <br/>
				<span class="italic">New SOTA on SimplerEnv-WidowX! Our SP-VLA simply converts your world models into a structured & middle-level robotic planner by predicting manipulation-relevant frames, unifying semantic planning with precise short-horizon control.</span> <br/>
                <!-- <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a>  -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/self_reflection_hallucinations.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2509.23236">Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection</a><br/>
                <span class="bold">Mingfei Han</span>, Haihong Hao, Jinxing Zhou, Zhihui Li, Yuhui Zheng, Xueqing Deng, Linjie Yang and Xiaojun Chang<br/>                
				<span class="italic">arxiv</span>, 2025 <br/>
		<span class="italic">Boosted by free self-reflection signals, our approach largely mitigates vision-language hallucinations, while also achieving improvements over general instruction-following benchmarks! </span> <br/>
                <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a> 
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/roomtour3d_generation.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://roomtour3d.github.io/">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a><br/>
                <span class="bold">Mingfei Han</span>, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang and Ivan Laptev<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition 2025 (<a href="https://cvpr.thecvf.com/">CVPR</a>)</span>, 2025 <br/>
		<span class="italic">Web-video based video-instruction(-action) training data. Effective, Automatic and Scalable! </span> <br/>
                <a class="btn btn-orange" href="https://roomtour3d.github.io">project page</a> / <a class="btn" href="http://arxiv.org/pdf/2412.08591">paper</a> / <a class="btn" href="./files/roomtour3d-slides.pdf">slides</a> / <a class="btn" href="https://github.com/roomtour3d/roomtour3d-NaviLLM">code</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/roomtour3d">Annotations</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/room_tour_video_3fps">Video Frames</a> / <a class="btn" href="https://huggingface.co/roomtour3d/roomtour3d-navillm-models">Models</a> / <a class="btn btn-dark" href="bibtex/han2024roomtour3d.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/sum_shot_model.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/shot2story">Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojun Chang, Lina Yao and Heng Wang<br/>
                <span class="italic">International Conference on Learning Representations (<a href="https://iclr.cc/Conferences/2025">ICLR</a>)</span>, 2025 <br/>
                <span class="italic">We present a new multi-shot video understanding benchmark Shot2Story with detailed shot-level captions, comprehensive video summaries and diverse question-answering pairs. 43K human annotations (with per-shot annotated visual and audio captions) + 90K GPTV annotations.</span><br/>
                <a class="btn btn-orange" href="https://mingfei.info/shot2story">project page</a> / <a class="btn" href="https://mingfei.info/files/paper_shot2story20k.pdf">paper</a> / <a class="btn" href="./files/shot2story_webnar-slides.pdf">slides</a> / <a class="btn" href="https://huggingface.co/spaces/mhan/Shot2Story">demo</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/">code</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md">data</a> / <a class="btn" href="https://www.youtube.com/watch?v=uhgUrxo1r80">video</a> / <a class="btn btn-dark" href="bibtex/han2023shot.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/malmm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://malmm1.github.io/">MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation</a><br/>
                Harsh Singh, Rocktim Jyoti Das, <span class="bold">Mingfei Han</span>, Preslav Nakov, Ivan Laptev<br/>
                <span class="italic">IEEE/RSJ International Conference on Intelligent Robots and Systems (<a href="https://www.iros25.org">IROS</a>)</span>, 2025 <br/>
                <span class="italic">A novel multi-agent framework to perform zero-shot object manipulation. Effective and Extendable! </span> <br/>
                <a class="btn btn-orange" href="https://malmm1.github.io/">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2411.17636">pdf</a> / <a class="btn btn-dark" href="bibtex/singh2024malmm.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/longvlm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2404.03384">LongVLM: Efficient Long Video Understanding via Large Language Models</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">European Conference on Computer Vision (<a href="https://eccv2024.ecva.net/">ECCV</a>)</span>, 2024 (<span class="bold" style="color: #f09228;">Oral</span>) <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/LongVLM">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2404.03384.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2024longvlm.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/PMV_glance.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/PMV">Video Recognition in Portrait Mode</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang and Heng Wang<br/>
                <span class="italic">We have developed the first dataset dedicated to portrait mode videos and focus on the research of this emerging video format.</span> CVPR 2024 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/PMV">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2312.13746">paper</a> / <a class="btn" href="github.com/bytedance/Portrait-Mode-Video">data</a> / <a class="btn btn-dark" href="bibtex/han2023pmv.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/mpvss.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://neurips.cc/virtual/2023/poster/72737">Mask Propagation for Efficient Video Semantic Segmentation</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">Thirty-seventh Conference on Neural Information Processing Systems (<a href="https://nips.cc/Conferences/2023">NeurIPS</a>)</span>, 2023 <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/MPVSS">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2310.18954.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/html.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="files/ICCV_RVOSv2.pdf">HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Zhihui Li, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">International Conference on Computer Vision (<a href="https://iccv2023.thecvf.com/">ICCV</a>)</span>, 2023 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/HTML">project page</a> / <a class="btn btn-red" href="files/ICCV_RVOSv2.pdf">pdf</a> / <a class="btn" href="files/ICCV_poster_HTML.pdf">poster</a> / <a class="btn btn-dark" href="bibtex/han2023html.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dual-ai.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2204.02148">Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition</a><br/>
                <span class="bold">Mingfei Han</span>, David Junhao Zhang, Yali Wang, Ruiyan, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2022.thecvf.com/">CVPR</a>)</span>, 2022 (<span class="bold" style="color: #f09228;">Oral</span>)<br/>
                <a class="btn btn-orange" href="https://mingfei.info/Dual-AI">project page</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2204.02148">arXiv</a> / <a class="btn" href="files/5612-video-slides.pdf">slides</a> / <a class="btn" href="files/CVPR Poster Session1.2-ID 42b.pdf">poster</a> / <a class="btn" href="https://www.youtube.com/watch?v=ydZa1kStOtI">presentation</a> / <a class="btn btn-dark" href="bibtex/han2022dualai.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/pfpm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10438399/">Progressive Frame-Proposal Mining for Weakly Supervised Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Mingjie Li, Xiaojun Chang, Yi Yang and Yu Qiao<br/>
		<span class="italic">IEEE Transactions on Image Processing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>)</span>, 2021<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/document/10438399/">IEEE</a> / <a class="btn btn-dark" href="bibtex/han2024progressive.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/hvrnet.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">Mining Inter-Video Relations for Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Xiaojun Chang, Yu Qiao<br/>
		<span class="italic">European Conference on Computer Vision (<a href="https://eccv2020.eu/">ECCV</a>)</span>, 2020<br/>
                <a class="btn btn-red" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">ECVA</a> / <a class="btn" href="https://github.com/youthHan/HVR-Net">code</a> / <a class="btn btn-dark" href="bibtex/han2020mining.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/CFME.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2107.00644">Object tracking in satellite videos by improved correlation filters with motion estimations</a><br/>
                Shiyu Xuan, Shengyang Li, <span class="bold">Mingfei Han</span>, Xue Wan, Gui-song Xia<br/>
                <span class="italic">IEEE Transactions on Geoscience and Remote Sensing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36/">TGRS</a>)</span>, 2019<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/abstract/document/8880656">IEEE</a> / <a class="btn" href="https://github.com/SY-Xuan/CFME">code</a> / <a class="btn btn-dark" href="bibtex/xuan2019object.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/trecvid.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">MMVG-INF-Etrol@ TRECVID 2019: Activities in Extended Video</a><br/>
                Xiaojun Chang, Wenhe Liu, Po-Yao Huang, Changlin Li, Fengda Zhu, <span class="bold">Mingfei Han</span>, <span class="italic">et al.</span><br/>
		    <span class="italic"><span class="bold">First Prize</span> on Trecvid Activities in Extended Video (<a href="https://actev.nist.gov/">ActEV</a></span>) challenge, 2019<br/>
                <a class="btn btn-orange" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">NIST</a> / <a class="btn btn-dark" href="bibtex/chang2019mmvg.txt">bibtex</a>
            </div>
        </div>
    </div>
    <div>
        <h2 class="noselect">Talks</h2>
        <ul>
	    <li><a href="https://news.ustc.edu.cn/info/1055/93589.htm">USTC Mozi Forum</a> <span class="italic">"Towards Open-World Robust Multimodal Reasoning and Decision-Making"</span>, December 2025</li>
	    <li><a href="http://english.ict.cas.cn">Institute of Computing Technology</a> <span class="italic">"Towards Generalization in Robotics Navigation and Manipulation"</span>, July 2025</li>
	    <li><a href="https://www.kcl.ac.uk/research/issa">KCL - ISSA technical exchange event</a> <span class="italic">"Shot2Story: A Multi-shot Understanding Approach for Archive Videos"</span>, June 2025</li>
	    <li><a href="https://cvpr.thecvf.com/virtual/2025/workshop/32284">CVPR 2025 - EAI Workshop</a> <span class="italic">"SMM Challenge: Social Mobile Manipulation"</span>, June 2025</li>
	    <li><a href="https://www.youtube.com/playlist?list=PLvqwYT_ECloZPB2BsBerHXxMpLGr2xuw9">Multimodal Minds</a> <span class="italic">"Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"</span>, Mar 2025</li>
	    <li><a href="https://b23.tv/0TXtNNo">3DCVer</a> in Chinese, <span class="italic">"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation"</span>, Mar 2025</li>
	    <li><a href="http://www.csig.org.cn/">China Society of Image and Graphics - Guangdong Branch</a> in Chinese, <span class="italic">"CSIG-Guangdong CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, May 2022</li>
	    <li><a href="https://www.cvmart.net/">Jishi Live</a> in Chinese, with my firend <a href="https://xiangtaokong.github.io/">Xiangtao Kong</a> who's on Low-level Vision and Super-Resolution, <span class="italic">"CAS-SIAT CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, with recording <a href="https://b23.tv/Xs517BY">here</a>, April 2022</li>
            <li><a href="https://www.monash.edu/it/dsai/">ML and VL Seminar at Monash University</a>, <span class="italic">"Mining Inter-Video Proposal Relations for Video Object Detection"</span>, November 2020</li>
        </ul>
    </div>
<!--     <div>
        <h2 class="noselect">Press coverage</h2>
        I have been mentioned in various media in connection with my research and my stay at UC Berkeley. Here's a few selected articles:
        <div class="row clearfix" style="margin-top: 16px;">
            <a href="https://bair.berkeley.edu/blog/2021/02/25/ss-adaptation/" class="press row-media" style="background-image: url(files/bair.png)"></a>
            <a href="https://blog.deeplearning.ai/blog/the-batch-predicting-car-crashes-profiting-from-deepfakes-piloting-drone-swarms-grading-data" class="press row-media" style="background-image: url(https://i.imgur.com/eoASzgo.png)"></a>
            <a href="https://www.facebook.com/dtudk/posts/2710489052354301" class="press img-contain row-media" style="background-image: url(https://i.imgur.com/BQjpjtE.png)"></a>
            <a href="https://sn.dk/Koege-Onsdag/Ung-koegenser-skal-bekaempe-hvidvask-i-Californien/artikel/904070" class="press row-media" style="background-image: url(https://i.imgur.com/jCBGIFZ.png)"></a>
        </div>
    </div> -->
    <div>
        <h2 class="noselect">Academic service</h2>
        <p>
            Reviewer for journals: TPAMI, IJCV, TIP, TCSVT, TNNLS, TMM, KBS, TOMM, PR.
        </p>
        <p>
            Reviewer for conferences: CVPR, ICCV, ECCV, ICLR, ICML, NeurIPS, 3DV, ACM MM, ACCV.
        </p>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I typically respond within a few days.<br/>
        I can be contacted directly at <span class="bold">mhannku030</span> [at] <span class="bold">gmail</span>.com
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>
