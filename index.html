<head>
    <title>Mingfei Han @MBZUAI</title>
    <meta name="author" content="Mingfei Han">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Mingfei Han@UTS">
	<meta property="og:description" content="Postdoctoral Associate, MBZUAI; Ph.D., University of Technology Sydney">
    <meta property="og:image" content="https://mingfei.info/files/me.jpeg">
	<meta property="og:url" content="https://mingfei.info/">
<!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/uts-logo.png">
    <link rel="icon" type="image/png" href="files/uts-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Mingfei Han</h1>
            </div>
            <div class="header-subtitle">
                Ph.D, University of Technology Sydney
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=wJEoIXsAAAAJ&hl=en">Google Scholar</a> /
                <a class="btn" href="https://github.com/youthHan">GitHub</a> /
                <a class="btn" href="files/resume.pdf">Resume</a> /
                <a class="btn" href="https://www.linkedin.com/in/mingfei-han/">LinkedIn</a>
            </div>
        </div>
    </div>
</div>
<style>
    .section-spacing {
        margin-bottom: 10px; /* Adjusts space between sections */
    }

    .list-item-spacing {
        margin-bottom: 5px; /* Adjusts space between list items */
    }
</style>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am a postdoctoral researcher at <a href="https://mbzuai.ac.ae/">Mohamed Bin Zayed University of Artificial Intelligence</a>, advised by <a href="https://www.di.ens.fr/~laptev/">Prof. Ivan Laptev</a>. I obtained my Ph.D. degree from <a href="https://www.uts.edu.au/">University of Technology Sydney</a>, advised by <a href="https://xiaojun.ai/">Prof. Xiaojun Chang</a>. I also worked closely with <a href="https://hengcv.github.io/">Heng Wang</a>, <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>, and <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a> on various video-language projects at Bytedance Seed. 
            Before moving to UTS, I spent a wonderful two years in <a href="https://monash.edu/">Monash University</a>. Prior to my candidature, I was a visiting student at <a href="https://mmlab.siat.ac.cn/">MMLab, SIAT, Chinese Academy of Sciences</a>, where I was fortunate to work with <a href="https://http://mmlab.siat.ac.cn/yuqiao/">Prof. Yu Qiao</a>, and <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ&hl=zh-CN">Prof. Yali Wang</a>.<br/>
            I received my Master's degree from University of Chinese Academy of Sciences (<a href="https://english.ucas.ac.cn/">UCAS</a>) and my Bachelor's degree from Nankai University (<a href="https://en.nankai.edu.cn/">NKU</a>) with graduate honours.
        </p>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Recent Activities</h2>
	    <ul>
<!-- 		<li class="list-item-spacing" style="color: #ff4500;">
		    <span role="img" aria-label="Star">ðŸŒŸðŸŒŸ</span> <strong>I am currently on the job market for a research position.</strong> <span role="img" aria-label="Star">ðŸŒŸðŸŒŸ</span>
		</li> -->
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">IROS 2025</span>: We won the first place awards in <a href="https://internrobotics.shlab.org.cn/challenge/2025/">Intern Robotics Challenge Track#2 (Vision-and-Language Navigation in Physical Environments)</a> and <a href="https://robosense2025.github.io/track2">RoboSense Challenge Track#2 (Social Navigation)</a>! </li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">NeurIPS 2025</span>: Our <a href="https://johanan528.github.io/worldweaver_web/">WorldWeaver</a> and <a href="https://phyblock.github.io/">PhyBlock</a> are accepted to NeurIPS 2025! </li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">IROS 2025!</span> <a href="https://malmm1.github.io/">MALMM</a>: Extendable and Effective multi-agent framework for robotics manipulation.</li>
		<li class="list-item-spacing" ><a href="https://robomani-grail.github.io/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸGRAIL Challenge & WorkshopðŸŒŸ</a>-<a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>: Benchmarking generalization in robotics manipulation with held testing set and offline real-world setup.</li>
		<li class="list-item-spacing" ><a href="https://smm-challenge.github.io/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸSMM ChallengeðŸŒŸ</a>-<a href="https://embodied-ai.org/cvpr2025/">EAI Workshop CVPR 2025</a>: Benchmarking the capability of performing long-sequence complex tasks through social interactions.</li>
		<li class="list-item-spacing" ><a href="https://arxiv.org/pdf/2412.08591/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸCVPR 2025ðŸŒŸ</a> <a href="https://roomtour3d.github.io/">RoomTour3D</a>: Diverse and Scalable video-instruction(-action) data for embodied navigation (Vision-and-Language Navigation). Ongoing effort with current 200k instructions released. We achieve SOTA on SOON and REVERIE with this newly introduced data.</li>
		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸICLR 2025ðŸŒŸ</a> <a href="https://mingfei.info/shot2story/">Shot2Story</a>: Train a rich video summarizer and test it on multi-shot videos! Just release our manually annotated Multi-Shot Video Understanding Suite. Single-shot captions, Mult-shot summaries, Video question-answering pairs: 134K multishot videos covering over 548k video shots; Detailed text summaries with over 6M words! SOTA video QA performance with the derived rich text summaries.</li>
<!-- 		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/">Shot2Story-134K</a>: Manual 43K + GPTV 90K. 134K multishot videos covering over 548k video shots; Detailed text summaries with over 6M words! We have released this new video description dataset. With the assistance of LLM, our method achieves SOTA performance on zero-shot MSRVTT-QA.</li> -->
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">ECCV 2024 Oral</span> <a href="https://github.com/ziplab/LongVLM">LongVLM</a>: Efficient long-video frame encoding for large video language models.</li>
		<!-- <li class="list-item-spacing" >CVPR 2024 <a href="https://mingfei.info/PMV/">PMV-400</a>: Portrait-mode videos rock the social media! We have developed the first video dataset dedicated to the research of this emerging video format.</li> -->
<!-- 		<li class="list-item-spacing" >ICCV 2023 <a href="https://mingfei.info/HTML/">HTML</a>: One paper on language referring video object segmentation (RVOS) gets accepted. No additional cost during inference with performance largely boosted!</li> -->
<!-- 		<li class="list-item-spacing" >NeurIPS 2023: One paper on efficient video segmentation gets accepted.</li> -->
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div class="section-spacing">
        <div>
            <h2 class="noselect">Research interest</h2>
            <p>
	       My research lies at the interface of computer vision and robotics. I currently investigate large visionâ€“language models, summarising videos and analysing their hallucination behaviour, further extending their use beyond embodied agents. Recent work spans videoâ€“language understanding, with a focus on long video understanding, video grounding tasks such as Referring Video Object Segmentation, alongside visionâ€“language navigation and manipulation for robots. Prior projects addressed individual and group activity recognition and video object detection under both fully and weakly supervised regimes, while my masterâ€™s thesis centred on moving-object detection and multi-object tracking.
            </p>
         </div>
    </div>
    <div>
        <h2 class="noselect">Publications and preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/self_reflection_hallucinations.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2509.23236">Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection</a><br/>
                <span class="bold">Mingfei Han</span>, Haihong Hao, Jinxing Zhou, Zhihui Li, Yuhui Zheng, Xueqing Deng, Linjie Yang and Xiaojun Chang<br/>
                <span class="italic">arxiv</span>, 2025 <br/>
		<span class="italic">Boosted by free self-reflection signals, our approach largely mitigates vision-language hallucinations, while also achieving improvements over general instruction-following benchmarks! </span> <br/>
                <a class="btn" href="http://arxiv.org/pdf/2509.23236">paper</a> 
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/roomtour3d_generation.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://roomtour3d.github.io/">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a><br/>
                <span class="bold">Mingfei Han</span>, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang and Ivan Laptev<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition 2025 (<a href="https://cvpr.thecvf.com/">CVPR</a>)</span>, 2025 <br/>
		<span class="italic">Web-video based video-instruction(-action) training data. Effective, Automatic and Scalable! </span> <br/>
                <a class="btn btn-orange" href="https://roomtour3d.github.io">project page</a> / <a class="btn" href="http://arxiv.org/pdf/2412.08591">paper</a> / <a class="btn" href="./files/roomtour3d-slides.pdf">slides</a> / <a class="btn" href="https://github.com/roomtour3d/roomtour3d-NaviLLM">code</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/roomtour3d">Annotations</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/room_tour_video_3fps">Video Frames</a> / <a class="btn" href="https://huggingface.co/roomtour3d/roomtour3d-navillm-models">Models</a> / <a class="btn btn-dark" href="bibtex/han2024roomtour3d.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/sum_shot_model.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/shot2story">Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojun Chang, Lina Yao and Heng Wang<br/>
                <span class="italic">International Conference on Learning Representations (<a href="https://iclr.cc/Conferences/2025">ICLR</a>)</span>, 2025 <br/>
                <span class="italic">We present a new multi-shot video understanding benchmark Shot2Story with detailed shot-level captions, comprehensive video summaries and diverse question-answering pairs. 43K human annotations (with per-shot annotated visual and audio captions) + 90K GPTV annotations.</span><br/>
                <a class="btn btn-orange" href="https://mingfei.info/shot2story">project page</a> / <a class="btn" href="https://mingfei.info/files/paper_shot2story20k.pdf">paper</a> / <a class="btn" href="./files/shot2story_webnar-slides.pdf">slides</a> / <a class="btn" href="https://huggingface.co/spaces/mhan/Shot2Story">demo</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/">code</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md">data</a> / <a class="btn" href="https://www.youtube.com/watch?v=uhgUrxo1r80">video</a> / <a class="btn btn-dark" href="bibtex/han2023shot.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/malmm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://malmm1.github.io/">MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation</a><br/>
                Harsh Singh, Rocktim Jyoti Das, <span class="bold">Mingfei Han</span>, Preslav Nakov, Ivan Laptev<br/>
                <span class="italic">IEEE/RSJ International Conference on Intelligent Robots and Systems (<a href="https://www.iros25.org">IROS</a>)</span>, 2025 <br/>
                <span class="italic">A novel multi-agent framework to perform zero-shot object manipulation. Effective and Extendable! </span> <br/>
                <a class="btn btn-orange" href="https://malmm1.github.io/">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2411.17636">pdf</a> / <a class="btn btn-dark" href="bibtex/singh2024malmm.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/longvlm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2404.03384">LongVLM: Efficient Long Video Understanding via Large Language Models</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">European Conference on Computer Vision (<a href="https://eccv2024.ecva.net/">ECCV</a>)</span>, 2024 (<span class="bold" style="color: #f09228;">Oral</span>) <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/LongVLM">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2404.03384.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2024longvlm.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/PMV_glance.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/PMV">Video Recognition in Portrait Mode</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang and Heng Wang<br/>
                <span class="italic">We have developed the first dataset dedicated to portrait mode videos and focus on the research of this emerging video format.</span> CVPR 2024 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/PMV">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2312.13746">paper</a> / <a class="btn" href="github.com/bytedance/Portrait-Mode-Video">data</a> / <a class="btn btn-dark" href="bibtex/han2023pmv.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/mpvss.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://neurips.cc/virtual/2023/poster/72737">Mask Propagation for Efficient Video Semantic Segmentation</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">Thirty-seventh Conference on Neural Information Processing Systems (<a href="https://nips.cc/Conferences/2023">NeurIPS</a>)</span>, 2023 <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/MPVSS">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2310.18954.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/html.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="files/ICCV_RVOSv2.pdf">HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Zhihui Li, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">International Conference on Computer Vision (<a href="https://iccv2023.thecvf.com/">ICCV</a>)</span>, 2023 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/HTML">project page</a> / <a class="btn btn-red" href="files/ICCV_RVOSv2.pdf">pdf</a> / <a class="btn" href="files/ICCV_poster_HTML.pdf">poster</a> / <a class="btn btn-dark" href="bibtex/han2023html.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dual-ai.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2204.02148">Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition</a><br/>
                <span class="bold">Mingfei Han</span>, David Junhao Zhang, Yali Wang, Ruiyan, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2022.thecvf.com/">CVPR</a>)</span>, 2022 (<span class="bold" style="color: #f09228;">Oral</span>)<br/>
                <a class="btn btn-orange" href="https://mingfei.info/Dual-AI">project page</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2204.02148">arXiv</a> / <a class="btn" href="files/5612-video-slides.pdf">slides</a> / <a class="btn" href="files/CVPR Poster Session1.2-ID 42b.pdf">poster</a> / <a class="btn" href="https://www.youtube.com/watch?v=ydZa1kStOtI">presentation</a> / <a class="btn btn-dark" href="bibtex/han2022dualai.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/pfpm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10438399/">Progressive Frame-Proposal Mining for Weakly Supervised Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Mingjie Li, Xiaojun Chang, Yi Yang and Yu Qiao<br/>
		<span class="italic">IEEE Transactions on Image Processing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>)</span>, 2021<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/document/10438399/">IEEE</a> / <a class="btn btn-dark" href="bibtex/han2024progressive.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/hvrnet.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">Mining Inter-Video Relations for Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Xiaojun Chang, Yu Qiao<br/>
		<span class="italic">European Conference on Computer Vision (<a href="https://eccv2020.eu/">ECCV</a>)</span>, 2020<br/>
                <a class="btn btn-red" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">ECVA</a> / <a class="btn" href="https://github.com/youthHan/HVR-Net">code</a> / <a class="btn btn-dark" href="bibtex/han2020mining.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/CFME.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2107.00644">Object tracking in satellite videos by improved correlation filters with motion estimations</a><br/>
                Shiyu Xuan, Shengyang Li, <span class="bold">Mingfei Han</span>, Xue Wan, Gui-song Xia<br/>
                <span class="italic">IEEE Transactions on Geoscience and Remote Sensing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36/">TGRS</a>)</span>, 2019<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/abstract/document/8880656">IEEE</a> / <a class="btn" href="https://github.com/SY-Xuan/CFME">code</a> / <a class="btn btn-dark" href="bibtex/xuan2019object.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/trecvid.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">MMVG-INF-Etrol@ TRECVID 2019: Activities in Extended Video</a><br/>
                Xiaojun Chang, Wenhe Liu, Po-Yao Huang, Changlin Li, Fengda Zhu, <span class="bold">Mingfei Han</span>, <span class="italic">et al.</span><br/>
		    <span class="italic"><span class="bold">First Prize</span> on Trecvid Activities in Extended Video (<a href="https://actev.nist.gov/">ActEV</a></span>) challenge, 2019<br/>
                <a class="btn btn-orange" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">NIST</a> / <a class="btn btn-dark" href="bibtex/chang2019mmvg.txt">bibtex</a>
            </div>
        </div>
    </div>
    <div>
        <h2 class="noselect">Talks</h2>
        <ul>
	    <li><a href="http://english.ict.cas.cn">Institute of Computing Technology</a> <span class="italic">"Towards Generalization in Robotics Navigation and Manipulation"</span>, July 2025</li>
	    <li><a href="https://www.kcl.ac.uk/research/issa">KCL - ISSA technical exchange event</a> <span class="italic">"Shot2Story: A Multi-shot Understanding Approach for Archive Videos"</span>, June 2025</li>
	    <li><a href="https://cvpr.thecvf.com/virtual/2025/workshop/32284">CVPR 2025 - EAI Workshop</a> <span class="italic">"SMM Challenge: Social Mobile Manipulation"</span>, June 2025</li>
	    <li><a href="https://www.youtube.com/playlist?list=PLvqwYT_ECloZPB2BsBerHXxMpLGr2xuw9">Multimodal Minds</a> <span class="italic">"Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"</span>, Mar 2025</li>
	    <li><a href="https://b23.tv/0TXtNNo">3DCVer</a> in Chinese, <span class="italic">"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation"</span>, Mar 2025</li>
	    <li><a href="http://www.csig.org.cn/">China Society of Image and Graphics - Guangdong Branch</a> in Chinese, <span class="italic">"CSIG-Guangdong CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, May 2022</li>
	    <li><a href="https://www.cvmart.net/">Jishi Live</a> in Chinese, with my firend <a href="https://xiangtaokong.github.io/">Xiangtao Kong</a> who's on Low-level Vision and Super-Resolution, <span class="italic">"CAS-SIAT CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, with recording <a href="https://b23.tv/Xs517BY">here</a>, April 2022</li>
            <li><a href="https://www.monash.edu/it/dsai/">ML and VL Seminar at Monash University</a>, <span class="italic">"Mining Inter-Video Proposal Relations for Video Object Detection"</span>, November 2020</li>
        </ul>
    </div>
<!--     <div>
        <h2 class="noselect">Press coverage</h2>
        I have been mentioned in various media in connection with my research and my stay at UC Berkeley. Here's a few selected articles:
        <div class="row clearfix" style="margin-top: 16px;">
            <a href="https://bair.berkeley.edu/blog/2021/02/25/ss-adaptation/" class="press row-media" style="background-image: url(files/bair.png)"></a>
            <a href="https://blog.deeplearning.ai/blog/the-batch-predicting-car-crashes-profiting-from-deepfakes-piloting-drone-swarms-grading-data" class="press row-media" style="background-image: url(https://i.imgur.com/eoASzgo.png)"></a>
            <a href="https://www.facebook.com/dtudk/posts/2710489052354301" class="press img-contain row-media" style="background-image: url(https://i.imgur.com/BQjpjtE.png)"></a>
            <a href="https://sn.dk/Koege-Onsdag/Ung-koegenser-skal-bekaempe-hvidvask-i-Californien/artikel/904070" class="press row-media" style="background-image: url(https://i.imgur.com/jCBGIFZ.png)"></a>
        </div>
    </div> -->
    <div>
        <h2 class="noselect">Academic service</h2>
        <p>
            Reviewer for journals: TPAMI, IJCV, TIP, TCSVT, TNNLS, TMM, KBS, TOMM, PR.
        </p>
        <p>
            Reviewer for conferences: CVPR, ICCV, ECCV, ICLR, ICML, NeurIPS, 3DV, ACM MM, ACCV.
        </p>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I typically respond within a few days.<br/>
        I can be contacted directly at <span class="bold">mhannku030</span> [at] <span class="bold">gmail</span>.com
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>
