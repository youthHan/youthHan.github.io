<head>
    <title>Mingfei Han @MBZUAI</title>
    <meta name="author" content="Mingfei Han">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Mingfei Han@UTS">
	<meta property="og:description" content="Ph.D. student, University of Technology Sydney">
    <meta property="og:image" content="https://mingfei.info/files/me.jpeg">
	<meta property="og:url" content="https://mingfei.info/">
<!-- 	<meta name="twitter:card" content="summary_large_image"> -->
    <link rel="apple-touch-icon" href="files/uts-logo.png">
    <link rel="icon" type="image/png" href="files/uts-logo.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Mingfei Han</h1>
            </div>
            <div class="header-subtitle">
                Ph.D, University of Technology Sydney
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=wJEoIXsAAAAJ&hl=en">Google Scholar</a> /
                <a class="btn" href="https://github.com/youthHan">GitHub</a> /
                <a class="btn" href="files/resume.pdf">Resume</a> /
                <a class="btn" href="https://www.linkedin.com/in/mingfei-han/">LinkedIn</a>
            </div>
        </div>
    </div>
</div>
<style>
    .section-spacing {
        margin-bottom: 10px; /* Adjusts space between sections */
    }

    .list-item-spacing {
        margin-bottom: 5px; /* Adjusts space between list items */
    }
</style>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I'm a postdoctoral researcher at <a href="https://mbzuai.ac.ae/">Mohamed Bin Zayed University of Artificial Intelligence</a>, advised by <a href="https://www.di.ens.fr/~laptev/">Prof. Ivan Laptev</a>. I obtained my Ph.D. degree from <a href="https://www.uts.edu.au/">University of Technology Sydney</a>, advised by <a href="https://xiaojun.ai/">Prof. Xiaojun Chang</a>. I also work closely with <a href="https://hengcv.github.io/">Heng Wang</a>, <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>, and <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a> on various video-language projects at Bytedance. 
            Before moving to UTS, I spent a wonderful two years in <a href="https://monash.edu/">Monash University</a>. Prior to my candidature, I was a visiting student at <a href="https://mmlab.siat.ac.cn/">MMLab, SIAT, Chinese Academy of Sciences</a>, where I was fortunate to work with <a href="https://http://mmlab.siat.ac.cn/yuqiao/">Prof. Yu Qiao</a>, and <a href="https://scholar.google.com/citations?user=hD948dkAAAAJ&hl=zh-CN">Prof. Yali Wang</a>.<br/>
            I received my Master's degree from University of Chinese Academy of Sciences (<a href="https://english.ucas.ac.cn/">UCAS</a>) and my Bachelor's degree from Nankai University (<a href="https://en.nankai.edu.cn/">NKU</a>) with graduate honours.
        </p>
    </div>
    <div class="section-spacing">
	<div>
	    <h2 class="noselect">Recent Activities</h2>
	    <ul>
<!-- 		<li class="list-item-spacing" style="color: #ff4500;">
		    <span role="img" aria-label="Star">ðŸŒŸðŸŒŸ</span> <strong>I am currently on the job market for a research position.</strong> <span role="img" aria-label="Star">ðŸŒŸðŸŒŸ</span>
		</li> -->
		<li class="list-item-spacing" ><a href="https://arxiv.org/pdf/2412.08591/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸCVPR 2025ðŸŒŸ</a> <a href="https://roomtour3d.github.io/">RoomTour3D</a>: Diverse and Scalable video-instruction(-action) data for embodied navigation (Vision-and-Language Navigation). Ongoing effort with current 200k instructions released. We achieve SOTA on SOON and REVERIE with this newly introduced data.</li>
		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/" style="color: #f09228; text-decoration: none; font-weight: bold;">ðŸŒŸICLR 2025ðŸŒŸ</a> <a href="https://mingfei.info/shot2story/">Shot2Story</a>: Manually annotated Multi-Shot Video Understanding Suite. Single-shot captions, Mult-shot summaries, Video question-answering pairs.</li>
		<li class="list-item-spacing" ><span class="bold" style="color: #f09228;">New!</span> <a href="https://malmm1.github.io/">MALMM</a>: Extendable and Effective multi-agent framework for robotics manipulation.</li>
<!-- 		<li class="list-item-spacing" ><a href="https://mingfei.info/shot2story/">Shot2Story-134K</a>: Manual 43K + GPTV 90K. 134K multishot videos covering over 548k video shots; Detailed text summaries with over 6M words! We have released this new video description dataset. With the assistance of LLM, our method achieves SOTA performance on zero-shot MSRVTT-QA.</li> -->
		<li class="list-item-spacing" >ECCV 2024 <span class="bold" style="color: #f09228;">Oral</span> <a href="https://github.com/ziplab/LongVLM">LongVLM</a>: Efficient long-video frame encoding for large video language models.</li>
		<li class="list-item-spacing" >CVPR 2024 <a href="https://mingfei.info/PMV/">PMV-400</a>: Portrait-mode videos rock the social media! We have developed the first video dataset dedicated to the research of this emerging video format.</li>
<!-- 		<li class="list-item-spacing" >ICCV 2023 <a href="https://mingfei.info/HTML/">HTML</a>: One paper on language referring video object segmentation (RVOS) gets accepted. No additional cost during inference with performance largely boosted!</li> -->
<!-- 		<li class="list-item-spacing" >NeurIPS 2023: One paper on efficient video segmentation gets accepted.</li> -->
		<!-- Add more list items here if needed -->
	    </ul>
	</div>
    </div>
    <div class="section-spacing">
        <div>
            <h2 class="noselect">Research interest</h2>
            <p>
	       My research interests lie in computer vision and machine learning. Currently, I am focusing on large vision-language models and their application in robotics. I worked on video-language downstream tasks related to object and event prediction in videos, like Referring-VOS and video grounding. Previously, I worked on individual and group activity recognition, and video object detection with full and limited supervision. During my Master's thesis, I worked on moving object detection and tracking.
            </p>
         </div>
    </div>
    <div>
        <h2 class="noselect">Publications and preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/roomtour3d_generation.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://roomtour3d.github.io/">RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</a><br/>
                <span class="bold">Mingfei Han</span>, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang and Ivan Laptev<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition 2025 (<a href="https://cvpr.thecvf.com/">CVPR</a>)</span>, 2025 <br/>
		<span class="italic">Web-video based video-instruction(-action) training data. Effective, Automatic and Scalable! </span> <br/>
                <a class="btn btn-orange" href="https://roomtour3d.github.io">project page</a> / <a class="btn" href="http://arxiv.org/pdf/2412.08591">paper</a> / <a class="btn" href="./files/roomtour3d-slides.pdf">slides</a> / <a class="btn" href="https://github.com/roomtour3d/roomtour3d-NaviLLM">code</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/roomtour3d">Annotations</a> / <a class="btn" href="https://huggingface.co/datasets/roomtour3d/room_tour_video_3fps">Video Frames</a> / <a class="btn" href="https://huggingface.co/roomtour3d/roomtour3d-navillm-models">Models</a> / <a class="btn btn-dark" href="bibtex/han2024roomtour3d.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/sum_shot_model.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/shot2story">Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojun Chang, Lina Yao and Heng Wang<br/>
                <span class="italic">International Conference on Learning Representations (<a href="https://iclr.cc/Conferences/2025">ICLR</a>)</span>, 2025 <br/>
                <span class="italic">We present a new multi-shot video understanding benchmark Shot2Story with detailed shot-level captions, comprehensive video summaries and diverse question-answering pairs. 43K human annotations (with per-shot annotated visual and audio captions) + 90K GPTV annotations.</span><br/>
                <a class="btn btn-orange" href="https://mingfei.info/shot2story">project page</a> / <a class="btn" href="https://mingfei.info/files/paper_shot2story20k.pdf">paper</a> / <a class="btn" href="./files/shot2story_webnar-slides.pdf">slides</a> / <a class="btn" href="https://huggingface.co/spaces/mhan/Shot2Story">demo</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/">code</a> / <a class="btn" href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md">data</a> / <a class="btn" href="https://www.youtube.com/watch?v=uhgUrxo1r80">video</a> / <a class="btn btn-dark" href="bibtex/han2023shot.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/malmm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://malmm1.github.io/">MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation</a><br/>
                Harsh Singh, Rocktim Jyoti Das, <span class="bold">Mingfei Han</span>, Preslav Nakov, Ivan Laptev<br/>
                <span class="italic">A novel multi-agent framework to perform zero-shot object manipulation. Effective and Extendable! </span> 2024 <br/>
                <a class="btn btn-orange" href="https://malmm1.github.io/">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2411.17636">pdf</a> / <a class="btn btn-dark" href="bibtex/singh2024malmm.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/longvlm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2404.03384">LongVLM: Efficient Long Video Understanding via Large Language Models</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">European Conference on Computer Vision (<a href="https://eccv2024.ecva.net/">ECCV</a>)</span>, 2024 (<span class="bold" style="color: #f09228;">Oral</span>) <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/LongVLM">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2404.03384.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2024longvlm.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/PMV_glance.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://mingfei.info/PMV">Video Recognition in Portrait Mode</a><br/>
                <span class="bold">Mingfei Han</span>, Linjie Yang, Xiaojie Jin, Jiashi Feng, Xiaojun Chang and Heng Wang<br/>
                <span class="italic">We have developed the first dataset dedicated to portrait mode videos and focus on the research of this emerging video format.</span> CVPR 2024 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/PMV">project page</a> / <a class="btn" href="https://arxiv.org/pdf/2312.13746">paper</a> / <a class="btn" href="github.com/bytedance/Portrait-Mode-Video">data</a> / <a class="btn btn-dark" href="bibtex/han2023pmv.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/mpvss.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://neurips.cc/virtual/2023/poster/72737">Mask Propagation for Efficient Video Semantic Segmentation</a><br/>
                Yuetian Weng, <span class="bold">Mingfei Han</span>, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang and Bohan Zhuang<br/>
                <span class="italic">Thirty-seventh Conference on Neural Information Processing Systems (<a href="https://nips.cc/Conferences/2023">NeurIPS</a>)</span>, 2023 <br/>
                <a class="btn btn-red" href="https://github.com/ziplab/MPVSS">code</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2310.18954.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a>
<!--                 <a class="btn btn-red" href="files/mpvss.pdf">pdf</a> / <a class="btn btn-dark" href="bibtex/weng2023mpvss.txt">bibtex</a> -->
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/html.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="files/ICCV_RVOSv2.pdf">HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Zhihui Li, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">International Conference on Computer Vision (<a href="https://iccv2023.thecvf.com/">ICCV</a>)</span>, 2023 <br/>
                <a class="btn btn-orange" href="https://mingfei.info/HTML">project page</a> / <a class="btn btn-red" href="files/ICCV_RVOSv2.pdf">pdf</a> / <a class="btn" href="files/ICCV_poster_HTML.pdf">poster</a> / <a class="btn btn-dark" href="bibtex/han2023html.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dual-ai.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2204.02148">Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition</a><br/>
                <span class="bold">Mingfei Han</span>, David Junhao Zhang, Yali Wang, Ruiyan, Lina Yao, Xiaojun Chang and Yu Qiao<br/>
                <span class="italic">Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2022.thecvf.com/">CVPR</a>)</span>, 2022 (<span class="bold" style="color: #f09228;">Oral</span>)<br/>
                <a class="btn btn-orange" href="https://mingfei.info/Dual-AI">project page</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2204.02148">arXiv</a> / <a class="btn" href="files/5612-video-slides.pdf">slides</a> / <a class="btn" href="files/CVPR Poster Session1.2-ID 42b.pdf">poster</a> / <a class="btn" href="https://www.youtube.com/watch?v=ydZa1kStOtI">presentation</a> / <a class="btn btn-dark" href="bibtex/han2022dualai.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/pfpm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10438399/">Progressive Frame-Proposal Mining for Weakly Supervised Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Mingjie Li, Xiaojun Chang, Yi Yang and Yu Qiao<br/>
		<span class="italic">IEEE Transactions on Image Processing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>)</span>, 2021<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/document/10438399/">IEEE</a> / <a class="btn btn-dark" href="bibtex/han2024progressive.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/hvrnet.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">Mining Inter-Video Relations for Video Object Detection</a><br/>
                <span class="bold">Mingfei Han</span>, Yali Wang, Xiaojun Chang, Yu Qiao<br/>
		<span class="italic">European Conference on Computer Vision (<a href="https://eccv2020.eu/">ECCV</a>)</span>, 2020<br/>
                <a class="btn btn-red" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf">ECVA</a> / <a class="btn" href="https://github.com/youthHan/HVR-Net">code</a> / <a class="btn btn-dark" href="bibtex/han2020mining.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/CFME.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2107.00644">Object tracking in satellite videos by improved correlation filters with motion estimations</a><br/>
                Shiyu Xuan, Shengyang Li, <span class="bold">Mingfei Han</span>, Xue Wan, Gui-song Xia<br/>
                <span class="italic">IEEE Transactions on Geoscience and Remote Sensing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36/">TGRS</a>)</span>, 2019<br/>
                <a class="btn btn-red" href="https://ieeexplore.ieee.org/abstract/document/8880656">IEEE</a> / <a class="btn" href="https://github.com/SY-Xuan/CFME">code</a> / <a class="btn btn-dark" href="bibtex/xuan2019object.txt">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/trecvid.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">MMVG-INF-Etrol@ TRECVID 2019: Activities in Extended Video</a><br/>
                Xiaojun Chang, Wenhe Liu, Po-Yao Huang, Changlin Li, Fengda Zhu, <span class="bold">Mingfei Han</span>, <span class="italic">et al.</span><br/>
		    <span class="italic"><span class="bold">First Prize</span> on Trecvid Activities in Extended Video (<a href="https://actev.nist.gov/">ActEV</a></span>) challenge, 2019<br/>
                <a class="btn btn-orange" href="https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/mmvg-Informedia.pdf">NIST</a> / <a class="btn btn-dark" href="bibtex/chang2019mmvg.txt">bibtex</a>
            </div>
        </div>
    </div>
    <div>
        <h2 class="noselect">Talks</h2>
        <ul>
	    <li><a href="https://www.youtube.com/playlist?list=PLvqwYT_ECloZPB2BsBerHXxMpLGr2xuw9">Multimodal Minds</a> <span class="italic">"Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"</span>, Mar 2025</li>
	    <li><a href="https://b23.tv/0TXtNNo">3DCVer</a> in Chinese, <span class="italic">"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation"</span>, Mar 2025</li>
	    <li><a href="http://www.csig.org.cn/">China Society of Image and Graphics - Guangdong Branch</a> in Chinese, <span class="italic">"CSIG-Guangdong CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, May 2022</li>
	    <li><a href="https://www.cvmart.net/">Jishi Live</a> in Chinese, with my firend <a href="https://xiangtaokong.github.io/">Xiangtao Kong</a> who's on Low-level Vision and Super-Resolution, <span class="italic">"CAS-SIAT CVPR Papers sharing - Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition"</span>, with recording <a href="https://b23.tv/Xs517BY">here</a>, April 2022</li>
            <li><a href="https://www.monash.edu/it/dsai/">ML and VL Seminar at Monash University</a>, <span class="italic">"Mining Inter-Video Proposal Relations for Video Object Detection"</span>, November 2020</li>
        </ul>
    </div>
<!--     <div>
        <h2 class="noselect">Press coverage</h2>
        I have been mentioned in various media in connection with my research and my stay at UC Berkeley. Here's a few selected articles:
        <div class="row clearfix" style="margin-top: 16px;">
            <a href="https://bair.berkeley.edu/blog/2021/02/25/ss-adaptation/" class="press row-media" style="background-image: url(files/bair.png)"></a>
            <a href="https://blog.deeplearning.ai/blog/the-batch-predicting-car-crashes-profiting-from-deepfakes-piloting-drone-swarms-grading-data" class="press row-media" style="background-image: url(https://i.imgur.com/eoASzgo.png)"></a>
            <a href="https://www.facebook.com/dtudk/posts/2710489052354301" class="press img-contain row-media" style="background-image: url(https://i.imgur.com/BQjpjtE.png)"></a>
            <a href="https://sn.dk/Koege-Onsdag/Ung-koegenser-skal-bekaempe-hvidvask-i-Californien/artikel/904070" class="press row-media" style="background-image: url(https://i.imgur.com/jCBGIFZ.png)"></a>
        </div>
    </div> -->
    <div>
        <h2 class="noselect">Academic service</h2>
        <p>
            Reviewer for journals: TPAMI, IJCV, TIP, TCSVT, TNNLS, TMM, KBS, TOMM.
        </p>
        <p>
            Reviewer for conferences: CVPR, ICCV, ECCV, ICLR, ICML, 3DV, ACM MM, ACCV.
        </p>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I typically respond within a few days.<br/>
        I can be contacted directly at <span class="bold">mhannku030</span> [at] <span class="bold">gmail</span>.com
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        Thanks for the website design from Nicklas Hansen <a href="https://nicklashansen.github.io/">here</a>.
    </div>
</div>
